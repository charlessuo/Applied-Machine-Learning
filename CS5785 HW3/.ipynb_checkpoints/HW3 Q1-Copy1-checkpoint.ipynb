{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as pt\n",
    "import string\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy.linalg as la\n",
    "import random\n",
    "# np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# http://stackoverflow.com/questions/16050952/how-to-remove-all-the-punctuation-in-a-string-python\n",
    "# parsed into the data and removed punctuation, also lower the string\n",
    "# AKA: preprocessing\n",
    "def parseInFile(name):\n",
    "    with open (name) as f:\n",
    "        lines = f.readlines()\n",
    "    X_data = []\n",
    "    Y_label = []\n",
    "    translator = str.maketrans({key: None for key in string.punctuation})\n",
    "    for line in lines:\n",
    "        temp = line.split('\\t')\n",
    "        X_data.append(temp[0].lower().translate(translator))\n",
    "        temp2 = temp[1].split('\\n')\n",
    "        Y_label.append(temp2[0])\n",
    "        \n",
    "    return X_data, Y_label\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# part C\n",
    "def splitDataSet(data, label):\n",
    "    count_pos = 0\n",
    "    count_neg = 0\n",
    "    i = 0\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "    X_test = []\n",
    "    Y_test = []\n",
    "    while(i < 1000):\n",
    "        if(label[i] == '1' and count_pos < 400):\n",
    "            X_train.append(data[i])\n",
    "            Y_train.append(label[i])\n",
    "            count_pos = count_pos + 1\n",
    "        elif(label[i] == '0' and count_neg < 400):\n",
    "            X_train.append(data[i])\n",
    "            Y_train.append(label[i])\n",
    "            count_neg = count_neg + 1\n",
    "        else:\n",
    "            if(label[i] == '0'):\n",
    "                count_neg = count_neg + 1\n",
    "            if(label[i] == '1'):\n",
    "                count_pos = count_pos + 1\n",
    "            X_test.append(data[i])\n",
    "            Y_test.append(label[i])\n",
    "        i = i + 1\n",
    "    print ('The # postive review is ' + str(count_pos) + ', the # negative review is ' + str(count_neg))\n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# k means \n",
    "def kmeans(word_dict, X_norm_train, k, iteration):\n",
    "\n",
    "    size = len(word_dict)\n",
    "    n = len(X_norm_train)\n",
    "    # generate random number between 0-1 for n\n",
    "    c = []\n",
    "    for i in range(n):\n",
    "        c.append(random.randint(0, k-1))\n",
    "    # print (c)\n",
    "    for i in range(iteration):\n",
    "        m_0 = np.zeros(size)\n",
    "        m_1 = np.zeros(size)\n",
    "        for j in range(n):\n",
    "            if c[j] == 0:\n",
    "                m_0 += X_norm_train[j, :]\n",
    "            else:\n",
    "                m_1 += X_norm_train[j, :]\n",
    "        m_0 = m_0/float(n-sum(c))\n",
    "        m_1 = m_1/float(sum(c))\n",
    "        for j in range(n):\n",
    "            if la.norm(X_norm_train[j, :] - m_0) < la.norm(X_norm_train[j, :] -m_1):\n",
    "                c[j] = 0\n",
    "            else:\n",
    "                c[j] = 1\n",
    "\n",
    "    return c, m_0, m_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "X_AMZ, Y_AMZ = parseInFile('sentiment labelled sentences/amazon_cells_labelled.txt')\n",
    "X_YELP, Y_YELP = parseInFile('sentiment labelled sentences/yelp_labelled.txt')\n",
    "X_IMDB, Y_IMDB = parseInFile('sentiment labelled sentences/imdb_labelled.txt')\n",
    "print (len(Y_AMZ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The # postive review is 500, the # negative review is 500\n",
      "The # postive review is 500, the # negative review is 500\n",
      "The # postive review is 500, the # negative review is 500\n"
     ]
    }
   ],
   "source": [
    "AMZ_X_train, AMZ_Y_train, AMZ_X_test, AMZ_Y_test = splitDataSet(X_AMZ, Y_AMZ)\n",
    "YELP_X_train, YELP_Y_train, YELP_X_test, YELP_Y_test = splitDataSet(X_YELP, Y_YELP)\n",
    "IMDB_X_train, IMDB_Y_train, IMDB_X_test, IMDB_Y_test = splitDataSet(X_IMDB, Y_IMDB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# part D Bag of Words Implementation\n",
    "# Explain why we canâ€™t use testing set at this point\n",
    "\n",
    "# First, we combine all training data and test data\n",
    "\n",
    "X_train = AMZ_X_train + YELP_X_train + IMDB_X_train\n",
    "\n",
    "Y_train = AMZ_Y_train + YELP_Y_train + IMDB_Y_train\n",
    "\n",
    "X_test = AMZ_X_test + YELP_X_test + IMDB_X_test\n",
    "Y_test = AMZ_Y_test + YELP_Y_test + IMDB_Y_test\n",
    "\n",
    "print (\"training data length is \" + str(len(X_train)) + \", test data length is \" + str(len(X_test)))\n",
    "# creat dict first\n",
    "# stop = {'a':0, 'is':0, 'an':0, 'that':0,'this':0, 'are':0, 'they':0, 'was':0 , 'were':0, 'do':0, 'not':0\n",
    "#            ,'the':0}\n",
    "word_dict = {}\n",
    "dict_count = 0\n",
    "for i in range(len(X_train)):\n",
    "    temps = X_train[i].split()\n",
    "    for temp in temps:\n",
    "        if temp not in word_dict:\n",
    "            word_dict[temp] = dict_count\n",
    "            dict_count += 1\n",
    "\n",
    "\n",
    "\n",
    "# build the feature vector\n",
    "X_feature_train = []\n",
    "for i in range(len(X_train)):\n",
    "    vector = np.zeros(dict_count)\n",
    "    temps = X_train[i].split()\n",
    "    for temp in temps:\n",
    "        if temp in word_dict:\n",
    "            vector[word_dict[temp]] += 1\n",
    "    X_feature_train.append(vector)\n",
    "        \n",
    "X_feature_train = np.array(X_feature_train)\n",
    "print (X_feature_train.shape)\n",
    "\n",
    "\n",
    "X_feature_test = []\n",
    "for i in range(len(X_test)):\n",
    "    vector = np.zeros(dict_count)\n",
    "    temps = X_test[i].split()\n",
    "    for temp in temps:\n",
    "        if temp in word_dict:\n",
    "            vector[word_dict[temp]] += 1\n",
    "    X_feature_test.append(vector)\n",
    "X_feature_test = np.array(X_feature_test)\n",
    "print (X_feature_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  1. ...,  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print (X_feature_train[0])\n",
    "Xs = np.arange(dict_count)\n",
    "pt.figure()\n",
    "pt.title('Bag of Words for First Review In Train Set')\n",
    "pt.plot(Xs, X_feature_train[0,:])\n",
    "pt.show()\n",
    "print (X_feature_train[-1])\n",
    "pt.title('Bag of Words for Last Review in Train Set')\n",
    "pt.plot(Xs, X_feature_train[-1,:])\n",
    "pt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# e post processing now\n",
    "# I choose l1 normalization\n",
    "X_norm_train = np.zeros(X_feature_train.shape)\n",
    "for i in range(len(X_train)):\n",
    "    X_norm_train[i,:] = X_feature_train[i,:]/float(sum(X_feature_train[i,:]))\n",
    "# print (X_norm_train[0,:])\n",
    "X_norm_test = np.zeros(X_feature_test.shape)\n",
    "for i in range(len(X_test)):\n",
    "    X_norm_test[i,:] = X_feature_test[i,:]/float(sum(X_feature_test[i,:]))\n",
    "# print (X_norm_test[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c, m_0, m_1 = kmeans(word_dict, X_norm_train, 2, 20)\n",
    "size = len(word_dict)\n",
    "Xs = np.arange(size)\n",
    "pt.figure()\n",
    "pt.title('Bag of Words Mean Vector in Negative Review')\n",
    "pt.plot(Xs, m_0)\n",
    "pt.show()\n",
    "pt.title('Bag of Words Mean Vector in Postive Review')\n",
    "pt.plot(Xs, m_1)\n",
    "pt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_accuracy(c, Y_train):\n",
    "    count = 0\n",
    "    for i in range(len(c)):\n",
    "        if str(c[i])  == Y_train[i]:\n",
    "            count +=1\n",
    "    accuracy = count / float(len(c))\n",
    "    return (accuracy)\n",
    "print (get_accuracy(c, Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/modules/linear_model.html\n",
    "def get_key_words_and_accuracy(word_dict,X_norm_train, Y_train, X_norm_test, Y_test, threhold):\n",
    "   \n",
    "    lg = LogisticRegression()\n",
    "    lg.fit(X_norm_train, Y_train)\n",
    "    lg_pred = lg.predict(X_norm_test)\n",
    "    count = 0\n",
    "    for i in range(len(lg_pred)):\n",
    "        if lg_pred[i]  == Y_test[i]:\n",
    "            count +=1\n",
    "    accuracy = count / float(len(lg_pred))\n",
    "    print (\"logistic regression accuracy is \" + str(accuracy) + \"!\")\n",
    "\n",
    "    coeff = lg.coef_[0]\n",
    "    key_index = []\n",
    "    key_words = []\n",
    "    for i in range(len(coeff)):\n",
    "        if abs(coeff[i]) > threhold:\n",
    "            key_index.append(i)\n",
    "    for index in key_index:\n",
    "        for k in word_dict:\n",
    "            if word_dict[k] == index:\n",
    "                key_words.append(k)\n",
    "\n",
    "    return key_words\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "print (get_key_words_and_accuracy(word_dict, X_norm_train, Y_train, X_norm_test, Y_test,1.34))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#h) N-gram model\n",
    "# creat dict first\n",
    "gram_dict = {}\n",
    "gram_count = 0\n",
    "# iterate over the entire reviews\n",
    "for i in range(len(X_train)):\n",
    "    review = X_train[i].split()\n",
    "    for j in range(len(review) - 1):\n",
    "        gram_word = review[j] + \" \" + review[j + 1]\n",
    "        if gram_word not in gram_dict:\n",
    "            gram_dict[gram_word] = gram_count\n",
    "            gram_count += 1\n",
    "\n",
    "\n",
    "# build the feature vector\n",
    "X_gram_train = []\n",
    "for i in range(len(X_train)):\n",
    "    vector = np.zeros(gram_count)\n",
    "    review = X_train[i].split()\n",
    "    for j in range(len(review) - 1):\n",
    "        gram_word = review[j] + \" \" + review[j + 1]\n",
    "        if gram_word in gram_dict:\n",
    "            vector[gram_dict[gram_word]] += 1\n",
    "    X_gram_train.append(vector)\n",
    "        \n",
    "X_gram_train = np.array(X_gram_train)\n",
    "print (X_gram_train.shape)\n",
    "\n",
    "\n",
    "X_gram_test = []\n",
    "for i in range(len(X_test)):\n",
    "    vector = np.zeros(gram_count)\n",
    "    review = X_test[i].split()\n",
    "    for j in range(len(review) - 1):\n",
    "        gram_word = review[j] + \" \" + review[j + 1]\n",
    "        if gram_word in gram_dict:\n",
    "            vector[gram_dict[gram_word]] += 1\n",
    "    X_gram_test.append(vector)\n",
    "X_gram_test = np.array(X_gram_test)\n",
    "print (X_feature_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xs = np.arange(gram_count)\n",
    "pt.figure()\n",
    "pt.title('2-Gram for 39th Review In Train Set')\n",
    "pt.plot(Xs, X_gram_train[38,:])\n",
    "pt.show()\n",
    "print (X_feature_train[-1])\n",
    "pt.title('2-Gram for Last Review in Train Set')\n",
    "pt.plot(Xs, X_gram_train[-1,:])\n",
    "pt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xg_norm_train = np.zeros(X_gram_train.shape)\n",
    "for i in range(len(X_train)):\n",
    "    if float(sum(X_gram_train[i,:])) != 0:\n",
    "        Xg_norm_train[i,:] = X_gram_train[i,:]/float(sum(X_gram_train[i,:]))\n",
    "# print (X_norm_train[0,:])\n",
    "Xg_norm_test = np.zeros(X_gram_test.shape)\n",
    "for i in range(len(X_test)):\n",
    "    if float(sum(X_gram_test[i,:])) != 0:\n",
    "        Xg_norm_test[i,:] = X_gram_test[i,:]/float(sum(X_gram_test[i,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gc, gm_0, gm_1 = kmeans(gram_dict, Xg_norm_train, 2, 20)\n",
    "size = len(gram_dict)\n",
    "Xs = np.arange(size)\n",
    "pt.figure()\n",
    "pt.title('2-Gram Mean Vector in Negative Review')\n",
    "pt.plot(Xs, gm_0)\n",
    "pt.show()\n",
    "pt.title('2-Gram Mean Vector in Postive Review')\n",
    "pt.plot(Xs,gm_1)\n",
    "pt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "print (get_accuracy(gc, Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (\"the keywords for 2-Gram is : \")\n",
    "print (get_key_words_and_accuracy(gram_dict, Xg_norm_train, Y_train, Xg_norm_test, Y_test, .88))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PCA(X_norm_train, X_norm_test, dimension):\n",
    "    U, D, VT = la.svd(X_norm_train)\n",
    "    VqT = VT[:dimension]\n",
    "    X_train_pca = np.zeros((len(X_norm_train), dimension))\n",
    "    for i in range(len(X_norm_train)):\n",
    "        X_train_pca[i, :] = VqT.dot(X_norm_train[i, :])\n",
    "    X_test_pca = np.zeros((len(X_norm_test), dimension))\n",
    "    for i in range(len(X_norm_test)):\n",
    "        X_test_pca[i, :] = VqT.dot(X_norm_test[i, :])\n",
    "    \n",
    "    size = dimension\n",
    "    iterations = 20\n",
    "    n = len(X_train_pca)\n",
    "    c = []\n",
    "    for i in range(n):\n",
    "        c.append(random.randint(0, 1))\n",
    "    # print (c)\n",
    "    for i in range(iterations):\n",
    "        m_0 = np.zeros(size)\n",
    "        m_1 = np.zeros(size)\n",
    "        for j in range(n):\n",
    "            if c[j] == 0:\n",
    "                m_0 += X_train_pca[j, :]\n",
    "            else:\n",
    "                m_1 += X_train_pca[j, :]\n",
    "        m_0 = m_0/float(n-sum(c))\n",
    "        m_1 = m_1/float(sum(c))\n",
    "        for j in range(n):\n",
    "            if la.norm(X_train_pca[j, :] - m_0) < la.norm(X_train_pca[j, :] -m_1):\n",
    "                c[j] = 0\n",
    "            else:\n",
    "                c[j] = 1\n",
    "\n",
    "    return c, m_0, m_1, X_train_pca, X_test_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c_pca10, m_0_pca10, m_1_pca10, X_train_pca10, X_test_pca10 = PCA(X_norm_train, X_norm_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (get_accuracy(c_pca10, Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xs = np.arange(10)\n",
    "pt.figure()\n",
    "pt.title('PCA-10 for Mean Vector in Negative Review')\n",
    "pt.plot(Xs, m_0_pca10)\n",
    "pt.show()\n",
    "pt.title('PCA-10 for Mean Vector in Postive Review')\n",
    "pt.plot(Xs, m_1_pca10)\n",
    "pt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lr_accuracy(X_train_pca, Y_train, X_test_pca, Y_test):\n",
    "    lg = LogisticRegression()\n",
    "    lg.fit(X_train_pca, Y_train)\n",
    "    lg_pred = lg.predict(X_test_pca)\n",
    "    count = 0\n",
    "    for i in range(len(lg_pred)):\n",
    "        if lg_pred[i]  == Y_test[i]:\n",
    "            count +=1\n",
    "    accuracy = count / float(len(lg_pred))\n",
    "    return accuracy\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (\"logistic regression accuracy for PCA10 is \" + str(get_lr_accuracy(X_train_pca10, Y_train, X_test_pca10, Y_test)) + \"!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c_pca50, m_0_pca50, m_1_pca50, X_train_pca50, X_test_pca50 = PCA(X_norm_train, X_norm_test, 50)\n",
    "print (get_accuracy(c_pca50, Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xs = np.arange(50)\n",
    "pt.figure()\n",
    "pt.title('PCA-50 for Mean Vector in Negative Review')\n",
    "pt.plot(Xs, m_0_pca50)\n",
    "pt.show()\n",
    "pt.title('PCA-50 for Mean Vector in Postive Review')\n",
    "pt.plot(Xs, m_1_pca50)\n",
    "pt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (\"logistic regression for PCA50 accuracy is \" + str(get_lr_accuracy(X_train_pca50, Y_train, X_test_pca50, Y_test)) + \"!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c_pca100, m_0_pca100, m_1_pca100, X_train_pca100, X_test_pca100 = PCA(X_norm_train, X_norm_test, 100)\n",
    "print (get_accuracy(c_pca100, Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xs = np.arange(100)\n",
    "pt.figure()\n",
    "pt.title('PCA-100 for Mean Vector in Negative Review')\n",
    "pt.plot(Xs, m_0_pca100)\n",
    "pt.show()\n",
    "pt.title('PCA-100 for Mean Vector in Postive Review')\n",
    "pt.plot(Xs, m_1_pca100)\n",
    "pt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (\"logistic regression for PCA100 accuracy is \" + str(get_lr_accuracy(X_train_pca100, Y_train, X_test_pca100, Y_test)) + \"!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
